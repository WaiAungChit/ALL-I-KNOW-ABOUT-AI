{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNNR5hrbffWK"
      },
      "source": [
        "#What is Gemini AI?\n",
        "\n",
        "Gemini AI is a large language model (LLM) developed by Google DeepMind. It was released on December 6th, 2023. Gemini is a “multimodal” language model, meaning that it is trained on a massive dataset of text and code.\n",
        "\n",
        "\n",
        "##What is “multimodal” in Gemini AI ?\n",
        "\n",
        "“Multimodal” in Gemini AI refers to its ability to understand and process information from various sources, not just text.\n",
        "\n",
        "-  Gemini AI is trained not only based on text but also Text, Images, Pictures, Audio, Code and Other modalities.\n",
        "-  Gemini AI can respond to multimodal prompts, combining text with images, audio, or code to generate new content. It can translate between different modalities, for example, generating text descriptions of images or creating images based on text descriptions.\n",
        "-  It can even use multimodal understanding to perform tasks like answering complex questions that require information from multiple sources.\n",
        "Three Types of Gemini AI Model\n",
        "\n",
        "\n",
        "##Three Types of Gemini AI Model\n",
        "\n",
        "![Gemini Model Size](https://drive.google.com/uc?id=19PY6P61jadO3WC1v61vR6xPtJwXnqpb8)\n",
        "\n",
        "1.  Gemini Ultra: This is the largest and most capable model, designed to tackle highly complex tasks efficiently.\n",
        "2.  Gemini Pro: Ideal for scaling across a broad range of tasks, Gemini Pro offers exceptional performance and adaptability.\n",
        "3.  Gemini Nano: The most efficient model for on-device tasks like android phones, Gemini Nano ensures optimal performance while conserving resources.\n",
        "\n",
        "\n",
        "##The Easiest way to test Gemini AI.\n",
        "The easiest way to test gemini ai is using [Google AI Studio](https://makersuite.google.com/).\n",
        "\n",
        "\n",
        "##Documentation Reference\n",
        "[Gemini AI](https://ai.google.dev/docs)\n",
        "\n",
        "##Prompts and model tuning\n",
        "\n",
        "Gemini AI has three prompt types\n",
        "\n",
        "- Freeform prompts - These prompts offer an open-ended prompting experience for generating content and responses to instructions. You can use both images and text data for your prompts.\n",
        "- Structured prompts - This prompting technique lets you guide model output by providing a set of example requests and replies. Use this approach when you need more control over the structure of model output.\n",
        "- Chat prompts - Use chat prompts to build conversational experiences. This prompting technique allows for multiple input and response turns to generate output.\n",
        "\n",
        "Google AI Studio also lets you to change the behavior of a model, using a technique called **tuning**.\n",
        "\n",
        "Tuned model - Use this advanced technique to improve a model's responses for a specific task by providing more examples. Note that tuning is only available for legacy PaLM models. Turn on the Show legacy models option in Settings to enable this prompt.\n",
        "\n",
        "##Freeform prompt\n",
        "\n",
        "To create a multimodal prompt:\n",
        "\n",
        "1.  Navigate to Google AI Studio.\n",
        "2.  In the left panel, select Create new > Freeform prompt.\n",
        "3.  In the right column Model field, select a model that supports images, such as the Gemini Pro Vision model.\n",
        "\n",
        "![Freeprompt](https://drive.google.com/uc?id=119dQa12fZX668dYZUMImwt28lBu6HxTM)\n",
        "\n",
        "\n",
        "##Add a replaceable variable to the prompt\n",
        "\n",
        "Sometimes if you want to be able to dynamically change parts of a prompt. We can modify specific input by seleting Test Imput and then add testing prompts. And we can change and test by changing run setting arguments.\n",
        "\n",
        "![Freeprompt](https://drive.google.com/uc?id=1Q-vy69B3iH574hqG8xikIDbheFfwsk5H)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB7ty1edfnNJ"
      },
      "source": [
        "##Structured Prompt\n",
        "\n",
        "Structured prompts in Google AI Studio help you do just that combine instructions with examples to show the model the kind of output you want, rather than just telling it what to do. This kind of prompting, called **few-shot** prompting.\n",
        "\n",
        "It is useful when you want the model to stick to a consistent output format (i.e. structured json) or when it’s difficult to describe in words what you want the model to do (i.e. write in a particular style).\n",
        "\n",
        "To create a multimodal prompt:\n",
        "\n",
        "- Navigate to Google AI Studio.\n",
        "- In the left panel, select Create new > Freeform prompt. (Note: Only 500 prompts can be given as static)\n",
        "\n",
        "\n",
        "Example : *generates advertising copy for products*\n",
        "\n",
        "###To import examples from a file:\n",
        "\n",
        "- In the top, right corner of examples table, select Actions > Import examples.\n",
        "- In the dialog, select a CSV or Google Sheets file in your Google Drive, or upload from your computer.\n",
        "- In the import examples dialog, choose which columns to import and which to leave out. The dialog also lets you specify which data column imports to which table column in your structured prompt.\n",
        "\n",
        "![StructuredPrompt](https://drive.google.com/uc?id=1ncUvjbpNs_GpPqSzpNY5dl1KTsnueBQO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqwcxbKPfqL7"
      },
      "source": [
        "##Chat prompt\n",
        "\n",
        "To create a chat prompt\n",
        "- Navigate to Google AI Studio.\n",
        "- In the left panel, select Create new > Chat Prompt\n",
        "\n",
        "\n",
        "![StructuredPrompt](https://drive.google.com/uc?id=1OjURaf1ht-YbY1D55wzY6sjXpXuAWQH9)\n",
        "\n",
        "###Teach your bot to chat better\n",
        "Chat reply with the long text. It is not user friendly. So let's teach our chat by adding examples.\n",
        "\n",
        "\n",
        "![StructuredPrompt](https://drive.google.com/uc?id=1squYycaoHUy3T5rAi3-fbt_hc8v-rsS6)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u43n69xdfs6G"
      },
      "source": [
        "#Gemini API\n",
        "\n",
        "###How to get API Key\n",
        "\n",
        "[Gemini API Key](https://makersuite.google.com/app/apikey) Go to the link and click *Create API key in new project*.\n",
        "\n",
        "###Verify your API key with a curl command\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "API_KEY=\"ADD YOUR_API_KEY\"\n",
        "curl -H 'Content-Type: application/json' \\\n",
        "     -d '{\"contents\":[\n",
        "            {\"role\": \"user\",\n",
        "              \"parts\":[{\"text\": \"Give me five subcategories of jazz?\"}]}]}' \\\n",
        "     \"https://generativelanguage.googleapis.com/v1/models/gemini-pro:generateContent?key=${API_KEY}\"\n",
        "```\n",
        "\n",
        "###AI Models\n",
        "- Gemini is Google's latest generation of generative models, and goes beyond the capabilities of the PaLM family of models.\n",
        "- A key difference between the Gemini and PaLM models is that the Gemini vision model is able to handle image input. You can prompt Gemini models with text, or images, or both. PaLM models only handle text input and output.\n",
        "\n",
        "![AI Models](https://drive.google.com/uc?id=1cEVWNhRD2_SM1TqcbYQ2Al-L5rsHq6T2)\n",
        "\n",
        "##Gemini Model variations\n",
        "\n",
        "![AI Models](https://drive.google.com/uc?id=1FLI3WIozk8Ir11OigSdEnHdlENhDbz6y)\n",
        "![AI Models](https://drive.google.com/uc?id=1KPVNc0j_rEllG3ESgC8jrXWqhRb4oJFa)\n",
        "![AI Models](https://drive.google.com/uc?id=1psuapdQl5w0vPBjZe-rke5FIAmpPXVfu)\n",
        "![AI Models](https://drive.google.com/uc?id=1AI-rARBP8evvSm4m6lo1R4-utMiWhv1r)\n",
        "\n",
        "###Gemini Meta Data\n",
        "![AI Models](https://drive.google.com/uc?id=1B8MooryRyd57JTA9MtbSNFZ5294cji8Z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a605mHnXfygL"
      },
      "source": [
        "#Safety Settings & Guidance\n",
        "\n",
        "By default, safety settings block content with medium and/or high probability of being unsafe content across all 4 dimensions.\n",
        "\n",
        "The adjustable safety filters cover the following categories:\n",
        "\n",
        "- Harassment\n",
        "- Hate speech\n",
        "- Sexually explicit\n",
        "- Dangerous\n",
        "\n",
        "![AI ModelsSafety ](https://drive.google.com/uc?id=16bKGgULRxRYsRpMy8Tx6-qe6nz3tNHk2)\n",
        "\n",
        "Large language models (LLMs) are so useful that they’re creative tools that can address many different language tasks. Unfortunately, this also means that large language models can generate output that you don't expect, including text that's offensive, insensitive, or factually incorrect. Gemini AI is designed with [Google AI Principles](https://ai.google/responsibility/principles/)\n",
        "\n",
        "![AI ModelsSafety ](https://drive.google.com/uc?id=18W2Fyh07401rc0l7HRXQSD0c5C8ST2Ei)\n",
        "\n",
        "\n",
        "![AI ModelsSafety ](https://drive.google.com/uc?id=1ai8LY6y4kUTNVUyNqiL19E6lVuHyKX3C)\n",
        "\n",
        "When we build our own applications with LLMs we need to consider the following points\n",
        "\n",
        "- Understanding the safety risks of your application\n",
        "- Considering adjustments to mitigate safety risks\n",
        "- Performing safety testing appropriate to your use case\n",
        "- Soliciting feedback from users and monitoring usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO55JAUzf1Bi"
      },
      "source": [
        "#Prompt design 101 or Prompting\n",
        "\n",
        "Prompt design is the process of creating prompts that elicit the desired response from language models. Writing well structured prompts is an essential part of ensuring accurate, high quality responses from a language model.\n",
        "\n",
        "###What is Prompt?\n",
        "\n",
        "A prompt is a natural language request submitted to a language model to receive a response back. Prompts can contain questions, instructions, contextual information, examples, and partial input for the model to complete or continue.\n",
        "\n",
        "\n",
        "Every prompt you send to the model includes parameter values that control how the model generates a response.\n",
        "The most common model parameters are:\n",
        "\n",
        "1.  Max output tokens -  Specifies the maximum number of tokens that can be generated in the response (100 tokens correspond to roughly 60-80 words)\n",
        "\n",
        "2.  Temperature -  The temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic or less open-ended response, while higher temperatures can lead to more diverse or creative results.\n",
        "\n",
        "3.  topK - It changes how the model selects tokens for output. A topK of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a topK of 3 means that the next token is selected from among the 3 most probable using the temperature.\n",
        "\n",
        "4.  topP - It changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the topP value.  For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the topP value is 0.5, then the model will select either A or B as the next token by using the temperature and exclude C as a candidate. Default topP value is 0.95\n",
        "\n",
        "\n",
        "##Types of prompts\n",
        "\n",
        "1.  **Zero-shot prompt** - These prompts don't contain examples for the model to replicate. Zero-shot prompts essentially show the model's ability to complete the prompt without any additional examples or information. It means the model has to rely on its pre-existing knowledge to generate a plausible answer. Use zero-shot prompts to generate creative text formats, such as poems, code, scripts, musical pieces, email, letters, etc.\n",
        "\n",
        "\n",
        "2. **One-shot prompts** - These prompts provide the model with a single example to replicate and continue the pattern. This allows for the generation of predictable responses from the model.\n",
        "\n",
        "3. **Few-shot prompts** - These prompts provide the model with multiple examples to replicate. Use few-shot prompts to complete complicated tasks, such as synthesizing data based on a pattern\n",
        "\n",
        "\n",
        "### Good Prompt Design Strategies\n",
        "\n",
        "\n",
        "\n",
        "*   Give clear instructions\n",
        "*   Include examples\n",
        "*   Let the model complete partial input\n",
        "*   Prompt the model to format its response\n",
        "*   Add contextual information\n",
        "*   Add prefixes\n",
        "*   Experiment with different parameter values\n",
        "*   Use different phrasing\n",
        "*   Switch to an analogous task\n",
        "*   Change the order of prompt content\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA7U8FgJf379"
      },
      "source": [
        "#GeminiAI With Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PISnT1zcf_k5"
      },
      "outputs": [],
      "source": [
        "#Install gemini ai\n",
        "\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0iT62yAngBmJ"
      },
      "outputs": [],
      "source": [
        "#Import necessary packages\n",
        "\n",
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "TKk5cqkWgOn1"
      },
      "outputs": [],
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZOUJNKdghUw"
      },
      "outputs": [],
      "source": [
        "# Setup API Key\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtPc9gz6hEPd"
      },
      "outputs": [],
      "source": [
        "#List models\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8V1OYADhSbH"
      },
      "source": [
        "##Generate Text from Text Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPsQ4WJ1hOgP"
      },
      "outputs": [],
      "source": [
        "#Select the model\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3iCePikhSC0"
      },
      "outputs": [],
      "source": [
        "#Check the CPU Time\n",
        "%%time\n",
        "response = model.generate_content(\"What is the meaning of life?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTBHTYFqh0hO"
      },
      "outputs": [],
      "source": [
        "#Generate text output\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DixkJea7h6J5"
      },
      "outputs": [],
      "source": [
        "#Check the prompt feedback (Safety)\n",
        "\n",
        "response.prompt_feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-881hKaTiCEi"
      },
      "outputs": [],
      "source": [
        "#Check other possible outputs\n",
        "\n",
        "response.candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZeoPxB-iMSd"
      },
      "outputs": [],
      "source": [
        "#Stream the output\n",
        "\n",
        "%%time\n",
        "response = model.generate_content(\"What is the meaning of life?\", stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI3h9U7HiXF7"
      },
      "outputs": [],
      "source": [
        "#Stream the output\n",
        "#Note: Some response attributes are not available in streaming until you've iterated through all the response chunks.\n",
        "\n",
        "for chunk in response:\n",
        "  print(chunk.text)\n",
        "  print(\"_\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNDPMC6zjN9m"
      },
      "source": [
        "## Generate text from image and text inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv_AyvnjjQ2Q"
      },
      "outputs": [],
      "source": [
        "#Get Sample Image\n",
        "!curl -o image.jpg https://i.pinimg.com/originals/51/ba/7f/51ba7f2b243f804805eca35af937089e.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5cH_VsRjj9u"
      },
      "outputs": [],
      "source": [
        "#Load image using PIL.Image python package\n",
        "\n",
        "import PIL.Image\n",
        "\n",
        "img = PIL.Image.open('image.jpg')\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tjhCkt2kg9p"
      },
      "outputs": [],
      "source": [
        "#Select Gemini pro vision model for image input\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro-vision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJenDSQekp7M"
      },
      "outputs": [],
      "source": [
        "#Generate content from image\n",
        "\n",
        "response = model.generate_content(img)\n",
        "\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA2HHlf8lJcI"
      },
      "outputs": [],
      "source": [
        "#Give Text input and image together\n",
        "\n",
        "response = model.generate_content([\"Write a short, engaging blog post based on this picture. It should include a description of the fruit in the photo and talk about eating fruits every day is healthy.\", img], stream=True)\n",
        "response.resolve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgr34U5MlYj8"
      },
      "outputs": [],
      "source": [
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbfN0Oi_l3dP"
      },
      "source": [
        "## Chat conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "050MmteZlzbF"
      },
      "outputs": [],
      "source": [
        "#Start build chat using gemini pro\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "chat = model.start_chat(history=[])\n",
        "chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAy62Yi-mA_1"
      },
      "outputs": [],
      "source": [
        "#Start Chat\n",
        "\n",
        "response = chat.send_message(\"In one sentence, explain how AI can influence in future\")\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8o94GVlmPlZ"
      },
      "outputs": [],
      "source": [
        "#list chat history\n",
        "\n",
        "chat.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ4nwqCTmTa_"
      },
      "outputs": [],
      "source": [
        "#Use stream to continue senting message\n",
        "\n",
        "response = chat.send_message(\"Okay, how about a more detailed explanation to a high schooler?\", stream=True)\n",
        "\n",
        "for chunk in response:\n",
        "  print(chunk.text)\n",
        "  print(\"_\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY8sVN35mf-n"
      },
      "outputs": [],
      "source": [
        "#Markdown chat list\n",
        "\n",
        "for message in chat.history:\n",
        "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1lEcwAbmqAP"
      },
      "source": [
        "##Use Embeddings\n",
        "\n",
        "The embedding service in the Gemini API generates state-of-the-art embeddings for words, phrases, and sentences. The resulting embeddings can then be used for NLP tasks, such as semantic search, text classification and clustering among many others.\n",
        "\n",
        "Embedding is a technique used to represent information as a list of floating point numbers in an array. With Gemini, you can represent text (words, sentences, and blocks of text) in a vectorized form, making it easier to compare and contrast embeddings. For example, two texts that share a similar subject matter or sentiment should have similar embeddings, which can be identified through mathematical comparison techniques such as cosine similarity.\n",
        "\n",
        " embed_content methods to generate embeddings\n",
        "\n",
        "![AI Embedding ](https://drive.google.com/uc?id=1eHlaBX4K6Dum3f259xZaiinzW2dsuZ7R)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z7DcSDymn7X"
      },
      "outputs": [],
      "source": [
        "#Embedding for single string document retrieval\n",
        "#The retrieval_document task type is the only task that accepts a title.\n",
        "\n",
        "result = genai.embed_content(\n",
        "    model=\"models/embedding-001\",\n",
        "    content=\"What is the meaning of life?\",\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Tesing retriveal document embedding\")\n",
        "\n",
        "# 1 input > 1 vector output\n",
        "print(str(result['embedding'])[:50], '... TRIMMED]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SqSFVdgqXnx"
      },
      "outputs": [],
      "source": [
        "#To handle batches of strings, pass a list of strings in content:\n",
        "\n",
        "result = genai.embed_content(\n",
        "    model=\"models/embedding-001\",\n",
        "    content=[\n",
        "      'What is the meaning of life?',\n",
        "      'How much wood would a woodchuck chuck?',\n",
        "      'How does the brain work?'],\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of list of strings\")\n",
        "\n",
        "# A list of inputs > A list of vectors output\n",
        "for v in result['embedding']:\n",
        "  print(str(v)[:50], '... TRIMMED ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoCu9IEkqymn"
      },
      "outputs": [],
      "source": [
        "#List other candidates\n",
        "\n",
        "response.candidates[0].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmMHgqLqq2Gd"
      },
      "outputs": [],
      "source": [
        "#Pass embedded using candidates\n",
        "\n",
        "result = genai.embed_content(\n",
        "    model = 'models/embedding-001',\n",
        "    content = response.candidates[0].content)\n",
        "\n",
        "# 1 input > 1 vector output\n",
        "print(str(result['embedding'])[:50], '... TRIMMED ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnWArkgIrOU2"
      },
      "outputs": [],
      "source": [
        "#List chat history\n",
        "\n",
        "chat.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RhtQm85rCfg"
      },
      "outputs": [],
      "source": [
        "#Pass embedded using Chat history\n",
        "\n",
        "result = genai.embed_content(\n",
        "    model = 'models/embedding-001',\n",
        "    content = chat.history)\n",
        "\n",
        "# 1 input > 1 vector output\n",
        "for i,v in enumerate(result['embedding']):\n",
        "  print(str(v)[:50], '... TRIMMED...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMS3TfOyrzJu"
      },
      "source": [
        "##Safety Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUrIOQ7rr6eG"
      },
      "outputs": [],
      "source": [
        "#Test rude words safety setting\n",
        "\n",
        "response = model.generate_content('[Questionable rude words prompt here]')\n",
        "response.candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGvKEFjNsWKK"
      },
      "outputs": [],
      "source": [
        "response.prompt_feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaG8ONZtsmas"
      },
      "outputs": [],
      "source": [
        "#Change Safety Setting\n",
        "response = model.generate_content('[Questionable prompt here]',\n",
        "                                  safety_settings={'HARASSMENT':'block_none'})\n",
        "response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ozWaVwytpnN"
      },
      "source": [
        "## Encode messages\n",
        "\n",
        "This  offers a fully-typed equivalent to the previous example, so you can better understand the lower-level details regarding how the SDK encodes messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiX_hr1euN0L"
      },
      "outputs": [],
      "source": [
        "import google.ai.generativelanguage as glm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXwD8X7HuSWq"
      },
      "outputs": [],
      "source": [
        "#Create Encoded Messages\n",
        "\n",
        "#The SDK attempts to convert your message to a `glm.Content` object, which contains a list of `glm.Part` objects that each contain either:\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro-vision')\n",
        "response = model.generate_content(\n",
        "    glm.Content(\n",
        "        parts = [\n",
        "            glm.Part(text=\"Write a short, engaging blog post based on this picture.\"),\n",
        "            glm.Part(\n",
        "                inline_data=glm.Blob(\n",
        "                    mime_type='image/jpeg',\n",
        "                    data=pathlib.Path('image.jpg').read_bytes()\n",
        "                )\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFPtSWTsuV40"
      },
      "outputs": [],
      "source": [
        "response.resolve()\n",
        "\n",
        "to_markdown(response.text[:100] + \"... [TRIMMED] ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVxs904IusnN"
      },
      "source": [
        "##Multi-turn conversations\n",
        "\n",
        "genai.ChatSession is just a wrapper around GenerativeModel.generate_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CHCixUZu3gs"
      },
      "outputs": [],
      "source": [
        "#Create chat model\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "messages = [\n",
        "    {'role':'user',\n",
        "     'parts': [\"Briefly explain how AI can benefit to students\"]}\n",
        "]\n",
        "response = model.generate_content(messages)\n",
        "\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tY5phs_vB1K"
      },
      "outputs": [],
      "source": [
        "#Note: For multi-turn conversations, you need to send the whole conversation history with each request. The API is stateless.\n",
        "\n",
        "messages.append({'role':'model',\n",
        "                 'parts':[response.text]})\n",
        "\n",
        "messages.append({'role':'user',\n",
        "                 'parts':[\"Okay, how about a more detailed explanation to a high school student?\"]})\n",
        "\n",
        "response = model.generate_content(messages)\n",
        "\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuUa8prdvRTs"
      },
      "source": [
        "## Generation configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mu9lnJVvSyA"
      },
      "outputs": [],
      "source": [
        "#The generation_config argument allows you to modify the generation parameters. Every prompt you send to the model includes parameter values that control how the model generates responses.\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(\n",
        "    'Tell me a story about a magic backpack.',\n",
        "    generation_config=genai.types.GenerationConfig(\n",
        "        # Only one candidate for now.\n",
        "        candidate_count=1,\n",
        "        stop_sequences=['x'],\n",
        "        max_output_tokens=20,\n",
        "        temperature=1.0)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuUFKicfwVE5"
      },
      "outputs": [],
      "source": [
        "#text = response.text\n",
        "\n",
        "#if response.candidates[0].finish_reason.name == \"MAX_TOKENS\":\n",
        " #   text += '...'\n",
        "\n",
        "#to_markdown(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_2i6Di1vgyU"
      },
      "outputs": [],
      "source": [
        "response.prompt_feedback"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
